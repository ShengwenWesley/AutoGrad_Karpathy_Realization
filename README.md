# Acknowledgement

This is walkthrough of basic concepts for Back-Propagation (BP) Process and AutoGrad project, refering to the following resources. Great thanks to Andrew Karpathy. 

[1] Rumelhart, D., Hinton, G. & Williams, R. Learning representations by back-propagating errors. *Nature* **323**, 533–536 (1986). https://doi.org/10.1038/323533a0

[2] Andrew Karpathy, 
‘The spelled-out intro to neural networks and backpropagation: building micrograd’ (Youtube https://youtu.be/VMj-3S1tku0) , 
‘micorgrad’ (Github https://github.com/karpathy/micrograd). 

# Explanation
The autograd.ipynb and Blog Sample Creation.ipynb are the source code for AutoGrad (adjusted version from Adrew Karpathy). <br>
I also have a notion notes written for this @ <br> https://relic-soda-875.notion.site/BackPropagation-AutoGrad-165409cd6d35809a9991e889409b2979 <br>
Feel free to leave a issue/pr if you see any problems. 
